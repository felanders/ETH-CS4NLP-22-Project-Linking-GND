{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from preprocessing import clean_gt, clean_raw, label_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from preprocessing import clean_gt, clean_raw, label_entity\n",
    "split = pickle.load(open('data/train_test_eval_filenames_new.pkl', 'rb'))\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train = 94, test = 30, eval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# new_split = {\"train\":[], \"test\": [], \"eval\": []}\n",
    "# for key in split:\n",
    "#     for page in split[key]:\n",
    "#         mag = page.split(\"_\")[0].split(\"-\")[0]\n",
    "#         year = page.split(\"_\")[1]\n",
    "        \n",
    "#         if mag == \"dkm\" and (year == \"1941\" or year == \"2010\"):\n",
    "#             new_split[\"train\"].append(page)\n",
    "#         if (year == \"1990\"):\n",
    "#             new_split[\"test\"].append(page)\n",
    "#         if mag ==\"sbz\" and (year == \"1895\" or year == \"1940\" or year == \"1965\" or year == \"2010\"):\n",
    "#             new_split[\"train\"].append(page)\n",
    "\n",
    "# eval_set = random.sample(new_split[\"train\"], int(len(new_split[\"train\"])/10)) #set 10% of train aside for eval\n",
    "# for page in eval_set:\n",
    "#     new_split[\"train\"].remove(page)\n",
    "# new_split[\"eval\"] = eval_set\n",
    "\n",
    "# with open('train_test_eval_filenames_new.pkl', 'wb') as out:\n",
    "#     pickle.dump(new_split, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data = {\n",
    "    \"train\": [],\n",
    "    \"test\": [],\n",
    "    \"eval\": []\n",
    "}\n",
    "gt_data = []\n",
    "for mag in [\"dkm\", \"sbz\"]:\n",
    "    for year in os.listdir(f'data/raw/link/{mag}'):\n",
    "        with open(os.path.join(\"data/raw/link\", mag, year)) as f:\n",
    "            input_linked = json.load(f)\n",
    "        with open(os.path.join(\"data/ground-truth\", mag, year)) as f:\n",
    "            gt = json.load(f)\n",
    "        gt = clean_gt(gt)\n",
    "        gt_data += gt\n",
    "        input_linked = clean_raw(input_linked)\n",
    "\n",
    "        #due to non-determinism in the flair NER:\n",
    "        all_refs_gt = [g[\"page\"]+g[\"coord\"] for g in gt] \n",
    "        all_refs_linked = [ent[\"page\"]+ent[\"coord\"] for l in input_linked for ent in l]\n",
    "        all_valid_refs = set(all_refs_gt).intersection(set(all_refs_linked))\n",
    "\n",
    "        for ent_variations in input_linked:\n",
    "            for key in split:\n",
    "                ent_instances = []\n",
    "                for ent in ent_variations:\n",
    "                    if ent[\"page\"] in split[key]:\n",
    "                        if (ent[\"page\"]+ent[\"coord\"]) in all_valid_refs:\n",
    "                            ent_instances.append({\"ent\": ent, \"label\": label_entity(ent, gt)})\n",
    "                if ent_instances:\n",
    "                    data[key].append(ent_instances)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open(\"data/processed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the relevant fastttext model uncomment and run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.fasttext import FastText, load_facebook_vectors\n",
    "# model = load_facebook_vectors(\"cc.de.300.bin/cc.de.300.bin\")\n",
    "# model.save(\"./fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from feature_generation import candidates_to_features, process_fuseki_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from feature_generation import candidates_to_features, process_fuseki_candidates\n",
    "#AF:\n",
    "from tqdm import tqdm\n",
    "# load everytime you run this as we pop keys to keep data clean..\n",
    "with open(\"data/processed.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "problematic_entities = []\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    ent_cand_label = []\n",
    "    i = 0\n",
    "    for entity_list in tqdm(data[split], smoothing=0.01):\n",
    "        i += 1\n",
    "        # Create candidates only for the first entry in the list as all the entity information is always the same\n",
    "        # The only thing that changes are pages and page_coordinates\n",
    "        \n",
    "        # fuseki:\n",
    "        unique_candidate_dict = get_candidates_fuseki(entity_list[0][\"ent\"])\n",
    "        candidates = process_fuseki_candidates(unique_candidate_dict)\n",
    "        #print(process_fuseki_candidates(unique_candidate_dict))\n",
    "        #list_of_tuples.append((ent, processed_fuseki_cands, ent_dict[\"label\"]))\n",
    "        \n",
    "        #metagrid\n",
    "        ## candidates = create_metagrid_candidates(ent=entity_list[0][\"ent\"])\n",
    "        # Generate the list of page_coordinates and the corresponding labels!\n",
    "        coord_list = []\n",
    "        gt_label = []\n",
    "        for ent_dict in entity_list:\n",
    "            ent = ent_dict[\"ent\"]\n",
    "            coord_list.append({\n",
    "                \"page\": ent.pop(\"page\", \"\"), \n",
    "                \"coords\": ent.pop(\"coord\", \"\")\n",
    "            })\n",
    "            gt_label.append(ent_dict[\"label\"])\n",
    "\n",
    "        #if len(gt_label)!=1:\n",
    "        #    problematic_entities.append({\"ent_list\": entity_list, \"gt_labels\": gt_label, \"mag\": coord_list})\n",
    "        #gt_label = gt_label.pop()\n",
    "        \n",
    "        ent_cand_label.append({\"entity\": ent, \"candidates\": candidates, \"occurences\": coord_list, \"gt_label\": gt_label})\n",
    "        if i % 100 == 0:\n",
    "            with open(f\"data/candidates/fuseki/candidates-gnd-{split}-{i}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(ent_cand_label, f)\n",
    "                    \n",
    "    with open(f\"data/candidates/fuseki/candidates-gnd-{split}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ent_cand_label, f)\n",
    "print(problematic_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import  tqdm\n",
    "from feature_generation import candidates_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = \"fuseki\" # or \"metagrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import  tqdm\n",
    "from feature_generation import candidates_to_features\n",
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from candidate_generation import create_metagrid_candidates, get_candidates_fuseki\n",
    "generator = \"fuseki\" # or \"metagrid\"\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"data/candidates/{generator}/candidates-gnd-{split}.pkl\", \"rb\") as f:\n",
    "        ent_cand_label = pickle.load(f)\n",
    "\n",
    "    list_of_good_entities = []\n",
    "    list_of_problematic_entities = []\n",
    "    for ent_dict in tqdm(ent_cand_label):\n",
    "        if len(set(ent_dict[\"gt_label\"])) > 1:\n",
    "            for label in set(ent_dict[\"gt_label\"]):\n",
    "                ent_dict[\"label\"] = label\n",
    "                features = candidates_to_features(ent=ent_dict[\"entity\"], candidates=ent_dict[\"candidates\"], gt_label=ent_dict[\"label\"])\n",
    "                ent_dict.update(features)\n",
    "                list_of_problematic_entities.append(ent_dict.copy())\n",
    "        else:\n",
    "            ent_dict[\"label\"] = set(ent_dict[\"gt_label\"]).pop()\n",
    "            features = candidates_to_features(ent=ent_dict[\"entity\"], candidates=ent_dict[\"candidates\"], gt_label=ent_dict[\"label\"])\n",
    "            ent_dict.update(features)\n",
    "            list_of_good_entities.append(ent_dict)\n",
    "            \n",
    "    with open(f\"data/features/{generator}/{split}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list_of_good_entities, file=f)\n",
    "    \n",
    "    with open(f\"data/features/{generator}/{split}_problematic.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list_of_problematic_entities, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Features\n",
    "\n",
    "The data is here:\n",
    "1. https://github.com/felanders/ETH-CS4NLP-22-Project-Linking-GND/tree/fatih/data/input/raw\n",
    "2. https://github.com/felanders/ETH-CS4NLP-22-Project-Linking-GND/tree/fatih/data/website_content_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import logging\n",
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "import importlib\n",
    "import unsupervised.raw_text_driver\n",
    "importlib.reload(unsupervised.raw_text_driver)\n",
    "import unsupervised.portal_dnb_driver\n",
    "importlib.reload(unsupervised.portal_dnb_driver)\n",
    "import unsupervised.data_loader\n",
    "with open(f\"data/features/train.pkl\", \"rb\") as f:\n",
    "    list_of_good_entities = pickle.load(f)\n",
    "\n",
    "importlib.reload(unsupervised.data_loader)\n",
    "\n",
    "# REPLACE THIS RAW DATA PATH\n",
    "data_loader = unsupervised.data_loader.DataLoader(raw_data_path='/home/aheser/ETH-CS4NLP-22-Project-Linking-GND/data/input/raw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = \"fuseki\" # or \"metagrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "avg_distance_counter = 0\n",
    "avg_distance = np.array((1,))\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "# for split in [\"eval\"]:\n",
    "    print('processing', split)\n",
    "    with open(f\"data/features/{generator}/{split}.pkl\", \"rb\") as f:\n",
    "        list_of_good_entities = pickle.load(f)\n",
    "    \n",
    "    some_counter = 0\n",
    "    for current_entity in tqdm(list_of_good_entities):\n",
    "        print(some_counter)\n",
    "        some_counter = some_counter + 1\n",
    "        distances = data_loader.get_context_distances(current_entity, similarity_measure='cosine_similarity', window_size=10)\n",
    "        for feature_counter in range(len(current_entity['features'])):\n",
    "            current_entity['features'][feature_counter].extend(distances[feature_counter])\n",
    "#             print('avg distance', avg_distance, 'distance vec', distances[feature_counter])\n",
    "            avg_distance = avg_distance + np.array(distances[feature_counter])\n",
    "            avg_distance_counter = avg_distance_counter + 1\n",
    "        \n",
    "    \n",
    "    with open(f\"data/features/{generator}/unsupervised_{split}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list_of_good_entities, file=f)\n",
    "\n",
    "avg_distance = avg_distance / avg_distance_counter\n",
    "# problematic ones (we don't have any vectors, use mean)\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    print('processing', split)\n",
    "    with open(f\"data/features/{generator}/{split}_problematic.pkl\", \"rb\") as f:\n",
    "        list_of_good_entities = pickle.load(f)\n",
    "    \n",
    "    for current_entity in tqdm(list_of_good_entities):\n",
    "        for feature_counter in range(len(current_entity['features'])):\n",
    "            current_entity['features'][feature_counter].extend(avg_distance)\n",
    "    \n",
    "    with open(f\"data/features/{generator}/unsupervised_{split}_problematic.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list_of_good_entities, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from evaluation import perform_experiment, crossvalidate_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = \"fuseki\" # or \"fuseki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"data/features/{generator}/unsupervised_{split}.pkl\", \"rb\") as f:\n",
    "        d[split] = pickle.load(file=f)\n",
    "\n",
    "# problematic entities\n",
    "d_problem = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"data/features/{generator}/unsupervised_{split}_problematic.pkl\", \"rb\") as f:\n",
    "        d_problem[split] = pickle.load(file=f)\n",
    "\n",
    "d_combined = {\"train\": d[\"train\"] + d_problem[\"train\"], \"eval\": d[\"eval\"] + d_problem[\"eval\"], \"test\": d[\"test\"] + d_problem[\"test\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best scores we could get**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_scores, ment_scores = perform_experiment(\n",
    "    keep_empty=True,\n",
    "    do_sample=True,\n",
    "    oversampling=3, # Multiple of how often we oversample y = 1\n",
    "    balance=3, # multiple of y = 0 samples vs y = 1 samples\n",
    "    train=d[\"train\"] + d[\"eval\"],\n",
    "    eval=d[\"test\"],\n",
    "    model=ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True),\n",
    "    n_s=[1,10], # How many candidates do we keep\n",
    "    thresholds=[0.01, 0.2], # Where do we cut off\n",
    "    verbose=False # Print stuff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEntity Level\")\n",
    "for score in ent_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()\n",
    "\n",
    "print(\"Mention Level\")\n",
    "for score in ment_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including problematic entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_scores, ment_scores = perform_experiment(\n",
    "    keep_empty=True,\n",
    "    do_sample=True,\n",
    "    oversampling=3, # Multiple of how often we oversample y = 1\n",
    "    balance=3, # multiple of y = 0 samples vs y = 1 samples\n",
    "    train=d_combined[\"train\"] + d_combined[\"eval\"],\n",
    "    eval=d_combined[\"test\"],\n",
    "    model=ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True),\n",
    "    n_s=[1,10], # How many candidates do we keep\n",
    "    thresholds=[0.01, 0.2], # Where do we cut off\n",
    "    verbose=False # Print stuff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEntity Level\")\n",
    "for score in ent_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()\n",
    "\n",
    "print(\"Mention Level\")\n",
    "for score in ment_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6d2888e1739c5fff784269431c26df89048cab5e87b70446ac408621b178c3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
