{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from preprocessing import clean_gt, clean_raw, label_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from preprocessing import clean_gt, clean_raw, label_entity\n",
    "split = pickle.load(open('data/train_test_eval_filenames_new.pkl', 'rb'))\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train = 94, test = 30, eval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# new_split = {\"train\":[], \"test\": [], \"eval\": []}\n",
    "# for key in split:\n",
    "#     for page in split[key]:\n",
    "#         mag = page.split(\"_\")[0].split(\"-\")[0]\n",
    "#         year = page.split(\"_\")[1]\n",
    "        \n",
    "#         if mag == \"dkm\" and (year == \"1941\" or year == \"2010\"):\n",
    "#             new_split[\"train\"].append(page)\n",
    "#         if (year == \"1990\"):\n",
    "#             new_split[\"test\"].append(page)\n",
    "#         if mag ==\"sbz\" and (year == \"1895\" or year == \"1940\" or year == \"1965\" or year == \"2010\"):\n",
    "#             new_split[\"train\"].append(page)\n",
    "\n",
    "# eval_set = random.sample(new_split[\"train\"], int(len(new_split[\"train\"])/10)) #set 10% of train aside for eval\n",
    "# for page in eval_set:\n",
    "#     new_split[\"train\"].remove(page)\n",
    "# new_split[\"eval\"] = eval_set\n",
    "\n",
    "# with open('train_test_eval_filenames_new.pkl', 'wb') as out:\n",
    "#     pickle.dump(new_split, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data = {\n",
    "    \"train\": [],\n",
    "    \"test\": [],\n",
    "    \"eval\": []\n",
    "}\n",
    "gt_data = []\n",
    "for mag in [\"dkm\", \"sbz\"]:\n",
    "    for year in os.listdir(f'data/raw/link/{mag}'):\n",
    "        with open(os.path.join(\"data/raw/link\", mag, year)) as f:\n",
    "            input_linked = json.load(f)\n",
    "        with open(os.path.join(\"data/ground-truth\", mag, year)) as f:\n",
    "            gt = json.load(f)\n",
    "        gt = clean_gt(gt)\n",
    "        gt_data += gt\n",
    "        input_linked = clean_raw(input_linked)\n",
    "\n",
    "        #due to non-determinism in the flair NER:\n",
    "        all_refs_gt = [g[\"page\"]+g[\"coord\"] for g in gt] \n",
    "        all_refs_linked = [ent[\"page\"]+ent[\"coord\"] for l in input_linked for ent in l]\n",
    "        all_valid_refs = set(all_refs_gt).intersection(set(all_refs_linked))\n",
    "\n",
    "        for ent_variations in input_linked:\n",
    "            for key in split:\n",
    "                ent_instances = []\n",
    "                for ent in ent_variations:\n",
    "                    if ent[\"page\"] in split[key]:\n",
    "                        if (ent[\"page\"]+ent[\"coord\"]) in all_valid_refs:\n",
    "                            ent_instances.append({\"ent\": ent, \"label\": label_entity(ent, gt)})\n",
    "                if ent_instances:\n",
    "                    data[key].append(ent_instances)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open(\"data/processed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the relevant fastttext model uncomment and run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.fasttext import FastText, load_facebook_vectors\n",
    "# model = load_facebook_vectors(\"cc.de.300.bin/cc.de.300.bin\")\n",
    "# model.save(\"./fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from feature_generation import candidates_to_features, process_fuseki_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from feature_generation import candidates_to_features, process_fuseki_candidates\n",
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from candidate_generation import create_metagrid_candidates, get_candidates_fuseki\n",
    "#AF:\n",
    "from tqdm import tqdm\n",
    "# load everytime you run this as we pop keys to keep data clean..\n",
    "with open(\"data/processed.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "problematic_entities = []\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    ent_cand_label = []\n",
    "    i = 0\n",
    "    for entity_list in tqdm(data[split], smoothing=0.01):\n",
    "        i += 1\n",
    "        # Create candidates only for the first entry in the list as all the entity information is always the same\n",
    "        # The only thing that changes are pages and page_coordinates\n",
    "        \n",
    "        # fuseki:\n",
    "        unique_candidate_dict = get_candidates_fuseki(entity_list[0][\"ent\"])\n",
    "        candidates = process_fuseki_candidates(unique_candidate_dict)\n",
    "        #print(process_fuseki_candidates(unique_candidate_dict))\n",
    "        #list_of_tuples.append((ent, processed_fuseki_cands, ent_dict[\"label\"]))\n",
    "        \n",
    "        #metagrid\n",
    "        ## candidates = create_metagrid_candidates(ent=entity_list[0][\"ent\"])\n",
    "        # Generate the list of page_coordinates and the corresponding labels!\n",
    "        coord_list = []\n",
    "        gt_label = []\n",
    "        for ent_dict in entity_list:\n",
    "            ent = ent_dict[\"ent\"]\n",
    "            coord_list.append({\n",
    "                \"page\": ent.pop(\"page\", \"\"), \n",
    "                \"coords\": ent.pop(\"coord\", \"\")\n",
    "            })\n",
    "            gt_label.append(ent_dict[\"label\"])\n",
    "\n",
    "        #if len(gt_label)!=1:\n",
    "        #    problematic_entities.append({\"ent_list\": entity_list, \"gt_labels\": gt_label, \"mag\": coord_list})\n",
    "        #gt_label = gt_label.pop()\n",
    "        \n",
    "        ent_cand_label.append({\"entity\": ent, \"candidates\": candidates, \"occurences\": coord_list, \"gt_label\": gt_label})\n",
    "        if i % 100 == 0:\n",
    "            with open(f\"data/candidates/fuseki/candidates-gnd-{split}-{i}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(ent_cand_label, f)\n",
    "                    \n",
    "    with open(f\"data/candidates/fuseki/candidates-gnd-{split}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ent_cand_label, f)\n",
    "print(problematic_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import  tqdm\n",
    "from feature_generation import candidates_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = \"fuseki\" # or \"metagrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import  tqdm\n",
    "from feature_generation import candidates_to_features\n",
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from candidate_generation import create_metagrid_candidates, get_candidates_fuseki\n",
    "generator = \"fuseki\" # or \"metagrid\"\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"data/candidates/{generator}/candidates-gnd-{split}.pkl\", \"rb\") as f:\n",
    "        ent_cand_label = pickle.load(f)\n",
    "\n",
    "    list_of_good_entities = []\n",
    "    list_of_problematic_entities = []\n",
    "    for ent_dict in tqdm(ent_cand_label):\n",
    "        if len(set(ent_dict[\"gt_label\"])) > 1:\n",
    "            for label in set(ent_dict[\"gt_label\"]):\n",
    "                ent_dict[\"label\"] = label\n",
    "                features = candidates_to_features(ent=ent_dict[\"entity\"], candidates=ent_dict[\"candidates\"], gt_label=ent_dict[\"label\"])\n",
    "                ent_dict.update(features)\n",
    "                list_of_problematic_entities.append(ent_dict.copy())\n",
    "        else:\n",
    "            ent_dict[\"label\"] = set(ent_dict[\"gt_label\"]).pop()\n",
    "            features = candidates_to_features(ent=ent_dict[\"entity\"], candidates=ent_dict[\"candidates\"], gt_label=ent_dict[\"label\"])\n",
    "            ent_dict.update(features)\n",
    "            list_of_good_entities.append(ent_dict)\n",
    "            \n",
    "    with open(f\"data/features/{generator}/{split}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list_of_good_entities, file=f)\n",
    "    \n",
    "    with open(f\"data/features/{generator}/{split}_problematic.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list_of_problematic_entities, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Features\n",
    "\n",
    "The data is here:\n",
    "1. https://github.com/felanders/ETH-CS4NLP-22-Project-Linking-GND/tree/fatih/data/input/raw\n",
    "2. https://github.com/felanders/ETH-CS4NLP-22-Project-Linking-GND/tree/fatih/data/website_content_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/features/train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(unsupervised\u001b[38;5;241m.\u001b[39mportal_dnb_driver)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munsupervised\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/features/train.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     list_of_good_entities \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     14\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(unsupervised\u001b[38;5;241m.\u001b[39mdata_loader)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/features/train.pkl'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import logging\n",
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "import importlib\n",
    "import unsupervised.raw_text_driver\n",
    "importlib.reload(unsupervised.raw_text_driver)\n",
    "import unsupervised.portal_dnb_driver\n",
    "importlib.reload(unsupervised.portal_dnb_driver)\n",
    "import unsupervised.data_loader\n",
    "with open(f\"data/features/train.pkl\", \"rb\") as f:\n",
    "    list_of_good_entities = pickle.load(f)\n",
    "\n",
    "importlib.reload(unsupervised.data_loader)\n",
    "\n",
    "# REPLACE THIS RAW DATA PATH\n",
    "data_loader = unsupervised.data_loader.DataLoader(raw_data_path='/home/aheser/ETH-CS4NLP-22-Project-Linking-GND/data/input/raw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = \"fuseki\" # or \"metagrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "avg_distance_counter = 0\n",
    "avg_distance = np.array((1,))\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "# for split in [\"eval\"]:\n",
    "    print('processing', split)\n",
    "    with open(f\"data/features/{generator}/{split}.pkl\", \"rb\") as f:\n",
    "        list_of_good_entities = pickle.load(f)\n",
    "    \n",
    "    some_counter = 0\n",
    "    for current_entity in tqdm(list_of_good_entities):\n",
    "        print(some_counter)\n",
    "        some_counter = some_counter + 1\n",
    "        distances = data_loader.get_context_distances(current_entity, similarity_measure='cosine_similarity', window_size=10)\n",
    "        for feature_counter in range(len(current_entity['features'])):\n",
    "            current_entity['features'][feature_counter].extend(distances[feature_counter])\n",
    "#             print('avg distance', avg_distance, 'distance vec', distances[feature_counter])\n",
    "            avg_distance = avg_distance + np.array(distances[feature_counter])\n",
    "            avg_distance_counter = avg_distance_counter + 1\n",
    "        \n",
    "    \n",
    "    with open(f\"data/features/{generator}/unsupervised_{split}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list_of_good_entities, file=f)\n",
    "\n",
    "avg_distance = avg_distance / avg_distance_counter\n",
    "# problematic ones (we don't have any vectors, use mean)\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    print('processing', split)\n",
    "    with open(f\"data/features/{generator}/{split}_problematic.pkl\", \"rb\") as f:\n",
    "        list_of_good_entities = pickle.load(f)\n",
    "    \n",
    "    for current_entity in tqdm(list_of_good_entities):\n",
    "        for feature_counter in range(len(current_entity['features'])):\n",
    "            current_entity['features'][feature_counter].extend(avg_distance)\n",
    "    \n",
    "    with open(f\"data/features/{generator}/unsupervised_{split}_problematic.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list_of_good_entities, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from evaluation import perform_experiment, crossvalidate_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = \"fuseki\" # or \"fuseki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/features/fuseki/unsupervised_train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}}\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/features/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgenerator\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/unsupervised_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         d[split] \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file\u001b[38;5;241m=\u001b[39mf)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# problematic entities\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/features/fuseki/unsupervised_train.pkl'"
     ]
    }
   ],
   "source": [
    "d = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"data/features/{generator}/unsupervised_{split}.pkl\", \"rb\") as f:\n",
    "        d[split] = pickle.load(file=f)\n",
    "\n",
    "# problematic entities\n",
    "d_problem = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"data/features/{generator}/unsupervised_{split}_problematic.pkl\", \"rb\") as f:\n",
    "        d_problem[split] = pickle.load(file=f)\n",
    "\n",
    "d_combined = {\"train\": d[\"train\"] + d_problem[\"train\"], \"eval\": d[\"eval\"] + d_problem[\"eval\"], \"test\": d[\"test\"] + d_problem[\"test\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best scores we could get**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_scores, ment_scores = perform_experiment(\n",
    "    keep_empty=True,\n",
    "    do_sample=True,\n",
    "    oversampling=3, # Multiple of how often we oversample y = 1\n",
    "    balance=3, # multiple of y = 0 samples vs y = 1 samples\n",
    "    train=d[\"train\"] + d[\"eval\"],\n",
    "    eval=d[\"test\"],\n",
    "    model=ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True),\n",
    "    n_s=[1,10], # How many candidates do we keep\n",
    "    thresholds=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5], # Where do we cut off\n",
    "    verbose=False # Print stuff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Only Non-Problematic Data - Entity Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.465 RE:  0.311 PR: 0.92 AC: 0.475\n",
      "TP: 46 FN: 102 FP 4 TN 50\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.1\n",
      "F1: 0.433 RE:  0.284 PR: 0.913 AC: 0.455\n",
      "TP: 42 FN: 106 FP 4 TN 50\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.2\n",
      "F1: 0.425 RE:  0.277 PR: 0.911 AC: 0.45\n",
      "TP: 41 FN: 107 FP 4 TN 50\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.3\n",
      "F1: 0.4 RE:  0.257 PR: 0.905 AC: 0.436\n",
      "TP: 38 FN: 110 FP 4 TN 50\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.4\n",
      "F1: 0.357 RE:  0.223 PR: 0.892 AC: 0.411\n",
      "TP: 33 FN: 115 FP 4 TN 50\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.5\n",
      "F1: 0.3 RE:  0.182 PR: 0.844 AC: 0.376\n",
      "TP: 27 FN: 121 FP 5 TN 49\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.517 RE:  0.351 PR: 0.981 AC: 0.52\n",
      "TP: 52 FN: 96 FP 1 TN 53\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.1\n",
      "F1: 0.487 RE:  0.324 PR: 0.98 AC: 0.5\n",
      "TP: 48 FN: 100 FP 1 TN 53\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.2\n",
      "F1: 0.48 RE:  0.318 PR: 0.979 AC: 0.495\n",
      "TP: 47 FN: 101 FP 1 TN 53\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.3\n",
      "F1: 0.448 RE:  0.291 PR: 0.977 AC: 0.475\n",
      "TP: 43 FN: 105 FP 1 TN 53\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.4\n",
      "F1: 0.398 RE:  0.25 PR: 0.974 AC: 0.446\n",
      "TP: 37 FN: 111 FP 1 TN 53\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.5\n",
      "F1: 0.343 RE:  0.209 PR: 0.939 AC: 0.411\n",
      "TP: 31 FN: 117 FP 2 TN 52\n",
      "\n",
      "\n",
      "Mention Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.573 RE:  0.431 PR: 0.856 AC: 0.558\n",
      "TP: 196 FN: 259 FP 33 TN 173\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.1\n",
      "F1: 0.539 RE:  0.396 PR: 0.845 AC: 0.534\n",
      "TP: 180 FN: 275 FP 33 TN 173\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.2\n",
      "F1: 0.537 RE:  0.393 PR: 0.844 AC: 0.533\n",
      "TP: 179 FN: 276 FP 33 TN 173\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.3\n",
      "F1: 0.519 RE:  0.376 PR: 0.838 AC: 0.52\n",
      "TP: 171 FN: 284 FP 33 TN 173\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.4\n",
      "F1: 0.491 RE:  0.349 PR: 0.828 AC: 0.502\n",
      "TP: 159 FN: 296 FP 33 TN 173\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.5\n",
      "F1: 0.442 RE:  0.305 PR: 0.799 AC: 0.469\n",
      "TP: 139 FN: 316 FP 35 TN 171\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.658 RE:  0.492 PR: 0.991 AC: 0.648\n",
      "TP: 224 FN: 231 FP 2 TN 204\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.1\n",
      "F1: 0.626 RE:  0.457 PR: 0.99 AC: 0.623\n",
      "TP: 208 FN: 247 FP 2 TN 204\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.2\n",
      "F1: 0.623 RE:  0.455 PR: 0.99 AC: 0.622\n",
      "TP: 207 FN: 248 FP 2 TN 204\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.3\n",
      "F1: 0.583 RE:  0.413 PR: 0.989 AC: 0.593\n",
      "TP: 188 FN: 267 FP 2 TN 204\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.4\n",
      "F1: 0.552 RE:  0.382 PR: 0.989 AC: 0.572\n",
      "TP: 174 FN: 281 FP 2 TN 204\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.5\n",
      "F1: 0.502 RE:  0.338 PR: 0.975 AC: 0.539\n",
      "TP: 154 FN: 301 FP 4 TN 202\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOnly Non-Problematic Data - Entity Level\")\n",
    "for score in ent_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()\n",
    "\n",
    "print(\"Mention Level\")\n",
    "for score in ment_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_scores, ment_scores = perform_experiment(\n",
    "    keep_empty=True,\n",
    "    do_sample=True,\n",
    "    oversampling=3, # Multiple of how often we oversample y = 1\n",
    "    balance=3, # multiple of y = 0 samples vs y = 1 samples\n",
    "    train=d[\"train\"] + d[\"eval\"],\n",
    "    eval=d[\"test\"],\n",
    "    model=ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True),\n",
    "    n_s=[1,10], # How many candidates do we keep\n",
    "    thresholds=[0.01, 0.1, 0.3, 0.5], # Where do we cut off\n",
    "    verbose=False # Print stuff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Only Non-Problematic Data - Entity Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.467 RE:  0.311 PR: 0.939 AC: 0.48\n",
      "TP: 46 FN: 102 FP 3 TN 51\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.1\n",
      "F1: 0.435 RE:  0.284 PR: 0.933 AC: 0.46\n",
      "TP: 42 FN: 106 FP 3 TN 51\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.3\n",
      "F1: 0.402 RE:  0.257 PR: 0.927 AC: 0.441\n",
      "TP: 38 FN: 110 FP 3 TN 51\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.5\n",
      "F1: 0.366 RE:  0.23 PR: 0.895 AC: 0.416\n",
      "TP: 34 FN: 114 FP 4 TN 50\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.522 RE:  0.358 PR: 0.964 AC: 0.52\n",
      "TP: 53 FN: 95 FP 2 TN 52\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.1\n",
      "F1: 0.485 RE:  0.324 PR: 0.96 AC: 0.495\n",
      "TP: 48 FN: 100 FP 2 TN 52\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.3\n",
      "F1: 0.446 RE:  0.291 PR: 0.956 AC: 0.47\n",
      "TP: 43 FN: 105 FP 2 TN 52\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.5\n",
      "F1: 0.394 RE:  0.25 PR: 0.925 AC: 0.436\n",
      "TP: 37 FN: 111 FP 3 TN 51\n",
      "\n",
      "\n",
      "Mention Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.575 RE:  0.431 PR: 0.863 AC: 0.561\n",
      "TP: 196 FN: 259 FP 31 TN 175\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.1\n",
      "F1: 0.541 RE:  0.396 PR: 0.853 AC: 0.537\n",
      "TP: 180 FN: 275 FP 31 TN 175\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.3\n",
      "F1: 0.518 RE:  0.374 PR: 0.846 AC: 0.522\n",
      "TP: 170 FN: 285 FP 31 TN 175\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.5\n",
      "F1: 0.498 RE:  0.356 PR: 0.831 AC: 0.507\n",
      "TP: 162 FN: 293 FP 33 TN 173\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.643 RE:  0.497 PR: 0.911 AC: 0.62\n",
      "TP: 226 FN: 229 FP 22 TN 184\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.1\n",
      "F1: 0.607 RE:  0.457 PR: 0.904 AC: 0.593\n",
      "TP: 208 FN: 247 FP 22 TN 184\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.3\n",
      "F1: 0.582 RE:  0.431 PR: 0.899 AC: 0.575\n",
      "TP: 196 FN: 259 FP 22 TN 184\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.5\n",
      "F1: 0.535 RE:  0.385 PR: 0.879 AC: 0.54\n",
      "TP: 175 FN: 280 FP 24 TN 182\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOnly Non-Problematic Data - Entity Level\")\n",
    "for score in ent_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()\n",
    "\n",
    "print(\"Mention Level\")\n",
    "for score in ment_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including problematic entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_scores, ment_scores = perform_experiment(\n",
    "    keep_empty=True,\n",
    "    do_sample=True,\n",
    "    oversampling=3, # Multiple of how often we oversample y = 1\n",
    "    balance=3, # multiple of y = 0 samples vs y = 1 samples\n",
    "    train=d_combined[\"train\"] + d_combined[\"eval\"],\n",
    "    eval=d_combined[\"test\"],\n",
    "    model=ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True),\n",
    "    n_s=[1,10], # How many candidates do we keep\n",
    "    thresholds=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5], # Where do we cut off\n",
    "    verbose=False # Print stuff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Only Combined Data - Entity Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.418 RE:  0.269 PR: 0.94 AC: 0.445\n",
      "TP: 47 FN: 128 FP 3 TN 58\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.1\n",
      "F1: 0.389 RE:  0.246 PR: 0.935 AC: 0.428\n",
      "TP: 43 FN: 132 FP 3 TN 58\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.2\n",
      "F1: 0.374 RE:  0.234 PR: 0.932 AC: 0.419\n",
      "TP: 41 FN: 134 FP 3 TN 58\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.3\n",
      "F1: 0.359 RE:  0.223 PR: 0.929 AC: 0.411\n",
      "TP: 39 FN: 136 FP 3 TN 58\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.4\n",
      "F1: 0.352 RE:  0.217 PR: 0.927 AC: 0.407\n",
      "TP: 38 FN: 137 FP 3 TN 58\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.5\n",
      "F1: 0.344 RE:  0.211 PR: 0.925 AC: 0.403\n",
      "TP: 37 FN: 138 FP 3 TN 58\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.478 RE:  0.314 PR: 1.0 AC: 0.492\n",
      "TP: 55 FN: 120 FP 0 TN 61\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.1\n",
      "F1: 0.451 RE:  0.291 PR: 1.0 AC: 0.475\n",
      "TP: 51 FN: 124 FP 0 TN 61\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.2\n",
      "F1: 0.438 RE:  0.28 PR: 1.0 AC: 0.466\n",
      "TP: 49 FN: 126 FP 0 TN 61\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.3\n",
      "F1: 0.416 RE:  0.263 PR: 1.0 AC: 0.453\n",
      "TP: 46 FN: 129 FP 0 TN 61\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.4\n",
      "F1: 0.402 RE:  0.251 PR: 1.0 AC: 0.445\n",
      "TP: 44 FN: 131 FP 0 TN 61\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.5\n",
      "F1: 0.372 RE:  0.229 PR: 1.0 AC: 0.428\n",
      "TP: 40 FN: 135 FP 0 TN 61\n",
      "\n",
      "\n",
      "Mention Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.676 RE:  0.527 PR: 0.942 AC: 0.593\n",
      "TP: 502 FN: 451 FP 31 TN 201\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.1\n",
      "F1: 0.661 RE:  0.51 PR: 0.94 AC: 0.58\n",
      "TP: 486 FN: 467 FP 31 TN 201\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.2\n",
      "F1: 0.655 RE:  0.503 PR: 0.939 AC: 0.574\n",
      "TP: 479 FN: 474 FP 31 TN 201\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.3\n",
      "F1: 0.652 RE:  0.499 PR: 0.939 AC: 0.571\n",
      "TP: 476 FN: 477 FP 31 TN 201\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.4\n",
      "F1: 0.65 RE:  0.497 PR: 0.939 AC: 0.57\n",
      "TP: 474 FN: 479 FP 31 TN 201\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.5\n",
      "F1: 0.647 RE:  0.494 PR: 0.938 AC: 0.567\n",
      "TP: 471 FN: 482 FP 31 TN 201\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.74 RE:  0.588 PR: 1.0 AC: 0.668\n",
      "TP: 560 FN: 393 FP 0 TN 232\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.1\n",
      "F1: 0.727 RE:  0.571 PR: 1.0 AC: 0.655\n",
      "TP: 544 FN: 409 FP 0 TN 232\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.2\n",
      "F1: 0.721 RE:  0.563 PR: 1.0 AC: 0.649\n",
      "TP: 537 FN: 416 FP 0 TN 232\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.3\n",
      "F1: 0.709 RE:  0.549 PR: 1.0 AC: 0.637\n",
      "TP: 523 FN: 430 FP 0 TN 232\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.4\n",
      "F1: 0.705 RE:  0.545 PR: 1.0 AC: 0.634\n",
      "TP: 519 FN: 434 FP 0 TN 232\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.5\n",
      "F1: 0.678 RE:  0.513 PR: 1.0 AC: 0.608\n",
      "TP: 489 FN: 464 FP 0 TN 232\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOnly Combined Data - Entity Level\")\n",
    "for score in ent_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()\n",
    "\n",
    "print(\"Mention Level\")\n",
    "for score in ment_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6d2888e1739c5fff784269431c26df89048cab5e87b70446ac408621b178c3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
