{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from preprocessing import clean_gt, clean_raw, label_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = pickle.load(open('/data/train_test_eval_filenames.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"train\": [],\n",
    "    \"test\": [],\n",
    "    \"eval\": []\n",
    "}\n",
    "gt_data = []\n",
    "for mag in [\"dkm\", \"sbz\"]:\n",
    "    for year in os.listdir(f'data/raw/link/{mag}'):\n",
    "        with open(os.path.join(\"data/raw/link\", mag, year)) as f:\n",
    "            input = json.load(f)\n",
    "        with open(os.path.join(\"data/ground-truth\", mag, year)) as f:\n",
    "            gt = json.load(f)\n",
    "        gt = clean_gt(gt)\n",
    "        gt_data += gt\n",
    "        input = clean_raw(input)\n",
    "        for ent_variations in input:\n",
    "            for key in split:\n",
    "                ent_instances = []\n",
    "                for ent in ent_variations:\n",
    "                    if ent[\"page\"] in split[key]:\n",
    "                        ent_instances.append({\"ent\": ent, \"label\": label_entity(ent, gt)})\n",
    "                if ent_instances:\n",
    "                    data[key].append(ent_instances)\n",
    "\n",
    "with open(\"/data/preprocessed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from candidate_generation import create_metagrid_candidates, get_coords_from_entitiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/preprocessed.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    ent_cand_label = []\n",
    "    i = 0\n",
    "    for entity_list in tqdm(data[split], smoothing=0.01):\n",
    "        i += 1\n",
    "        for ent_dict in entity_list:\n",
    "            list_of_tuples = []\n",
    "            ent = ent_dict[\"ent\"]\n",
    "            ent.update(get_coords_from_entitiy(ent))\n",
    "            list_of_tuples.append((ent, create_metagrid_candidates(ent=ent_dict[\"ent\"]), ent_dict[\"label\"]))\n",
    "        ent_cand_label.append(list_of_tuples)\n",
    "        if i % 100 == 0:\n",
    "            with open(f\"data/candidates/gnd-{split}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(ent_cand_label, f)\n",
    "                    \n",
    "    with open(f\"data/candidates/gnd-{split}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ent_cand_label, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from feature_generation import create_features, get_gnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"data/candidates/gnd-{split}.pkl\", \"rb\") as f:\n",
    "        ent_cand_label = pickle.load(f)\n",
    "    X = []\n",
    "    y = []\n",
    "    X_ent = []\n",
    "    y_ent = []\n",
    "\n",
    "    def get_gnd(candidate):\n",
    "        return candidate[\"Gnd\"]\n",
    "\n",
    "    for ent, candidates, label in tqdm(ent_cand_label):\n",
    "        y_ent_int = []\n",
    "        X_ent_int = []\n",
    "        for c in candidates:\n",
    "            features = create_features(ent, c)\n",
    "            X_ent_int.append([get_gnd(c)] + features)\n",
    "        y_ent.append([label])\n",
    "        X_ent.append(X_ent_int)\n",
    "    for filename, file in zip([f\"y_ent\", f\"X_ent\"], [y_ent, X_ent]):\n",
    "        with open(f\"data/features/{split}/{filename}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(file, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"train\": {}, \"eval\": {}}\n",
    "for split in [\"train\", \"eval\"]:\n",
    "    for filename in [\"y\", \"X\", \"y_ent\", \"X_ent\"]:\n",
    "        with open(f\"data/features/{split}/{filename}.pkl\", \"rb\") as f:\n",
    "            d[split][filename] = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "y_test_ids = []\n",
    "x_test_ids = []\n",
    "\n",
    "for x_ent, y_ent in zip(d[\"train\"][\"X_ent\"], d[\"train\"][\"y_ent\"]):\n",
    "    for x in x_ent:\n",
    "        X_train.append(x[1:])\n",
    "        if x[0] == y_ent[0]:\n",
    "            y_train.append(1)\n",
    "        else:\n",
    "            y_train.append(0)\n",
    "\n",
    "for x_ent, y_ent in zip(d[\"eval\"][\"X_ent\"], d[\"eval\"][\"y_ent\"]):\n",
    "    for x in x_ent:\n",
    "        X_test.append(x[1:])\n",
    "        if x[0] == y_ent[0]:\n",
    "            y_test.append(1)\n",
    "        else:\n",
    "            y_test.append(0)\n",
    "        x_test_ids.append(x[0])\n",
    "        y_test_ids.append(y_ent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train)\n",
    "df[\"y\"] = y_train\n",
    "\n",
    "def fun(x):\n",
    "    if x.shape[0] > 49:\n",
    "        return x.sample(frac = 0.1)\n",
    "    else:\n",
    "        return x.sample(147, replace=True)\n",
    "\n",
    "sample = df.groupby('y', group_keys=False).apply(lambda x: fun(x))\n",
    "\n",
    "y_sample = sample[\"y\"]\n",
    "x_sample = sample[[0, 1, 2, 3, 4, 5, 6, 7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesRegressor(bootstrap=True, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesRegressor</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesRegressor(bootstrap=True, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesRegressor(bootstrap=True, random_state=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "model = ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True)\n",
    "model.fit(x_sample, y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(candidates, model):\n",
    "    candidate_scores = []\n",
    "    candidate_ids = []\n",
    "    for candidate in candidates:\n",
    "        features = candidate[1:]\n",
    "        score = model.predict(np.array(features).reshape(1,-1))[0]\n",
    "        candidate_scores.append(score)\n",
    "        candidate_ids.append(candidate[0])\n",
    "    if candidate_ids:\n",
    "        candidate_ids = np.array(candidate_ids)\n",
    "        indices = np.argsort(candidate_scores)\n",
    "        ids_sorted = np.array(candidate_ids)[indices[::-1]]\n",
    "        scores_sorted = np.array(candidate_scores)[indices[::-1]]\n",
    "        return ids_sorted, scores_sorted\n",
    "    else:\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mention Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f063e963b0414a34a64aa3bf393c46c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "y_ids = []\n",
    "for x_ent, y_ent in tqdm(zip(d[split][\"X_ent\"], d[split][\"y_ent\"])):\n",
    "    ids_local, scores_local = rank_candidates(x_ent, model)\n",
    "    ids.append(ids_local)\n",
    "    scores.append(scores_local)\n",
    "    y_ids.append(y_ent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = 0.0001\n",
    "split = \"eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp, tn, fp, fn\n",
      "22 1311 1432 293\n",
      "F1:           0.0249\n",
      "Recall:       0.0698\n",
      "Precision:    0.0151\n",
      "Accuracy:     0.4359\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "for ids_local, scores_local, y in zip(ids, scores, y_ids):\n",
    "    for id, score in zip(ids_local, scores_local):\n",
    "        if score > treshold:\n",
    "            if id == y:\n",
    "                tp += 1\n",
    "            elif id == '':\n",
    "                fn += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        elif y == '':\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "print(\"tp, tn, fp, fn\")\n",
    "print(tp, tn, fp, fn)\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2* (precision * recall)/(precision+recall)\n",
    "print(\"F1:          \", round(f1,4))\n",
    "print(\"Recall:      \", round(recall,4))        \n",
    "print(\"Precision:   \", round(precision,4))\n",
    "print(\"Accuracy:    \", round(accuracy,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_prediction(ids, scores, cutoff_score=0.5, n_candidates=5):\n",
    "    count = 0\n",
    "    result = []\n",
    "    for id, score in zip(ids, scores):\n",
    "        count += 1\n",
    "        if count > n_candidates:\n",
    "            return result\n",
    "        else:\n",
    "            if score > cutoff_score:\n",
    "                result.append(id)\n",
    "    if len(result) < 1 :\n",
    "        result.append('')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b9195d424243f994a983fef7ffcf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "for x_ent in tqdm(d[split][\"X_ent\"]):\n",
    "    ids_local, scores_local = rank_candidates(x_ent, model)\n",
    "    ids.append(ids_local)\n",
    "    scores.append(scores_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = 0.5\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:           0.381\n",
      "Recall:       0.3922\n",
      "Precision:    0.3704\n",
      "Accuracy:     0.6328\n",
      "\n",
      "tp tn  fp fn\n",
      "20 92 34 31\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for ids_local, scores_local in zip(ids, scores):\n",
    "    prediction = normalize_prediction(ids_local, scores_local, cutoff_score=treshold, n_candidates = n)\n",
    "    predictions.append(prediction)\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "\n",
    "for prediction, y in zip(predictions, d[split][\"y_ent\"]):\n",
    "    if y[0] == '':\n",
    "        if y[0] in prediction:\n",
    "            tn +=1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if y[0] in prediction:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "accuracy = (tp +tn)/(tp +tn +fp +fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2* (precision * recall)/(precision +recall)\n",
    "print(\"F1:          \", round(f1,4))\n",
    "print(\"Recall:      \", round(recall,4))        \n",
    "print(\"Precision:   \", round(precision,4))\n",
    "print(\"Accuracy:    \", round(accuracy,4))\n",
    "print(\"\\ntp tn  fp fn\")\n",
    "print(tp, tn, fp, fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6d2888e1739c5fff784269431c26df89048cab5e87b70446ac408621b178c3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
