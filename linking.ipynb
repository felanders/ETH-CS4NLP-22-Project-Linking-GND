{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from preprocessing import clean_gt, clean_raw, label_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split = pickle.load(open('data/train_test_eval_filenames_new.pkl', 'rb'))\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Though we do have a train/eval split, we actually simply combine the two for the cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"train\": [],\n",
    "    \"test\": [],\n",
    "    \"eval\": []\n",
    "}\n",
    "gt_data = []\n",
    "for mag in [\"dkm\", \"sbz\"]:\n",
    "    for year in os.listdir(f'data/raw/link/{mag}'):\n",
    "        with open(os.path.join(\"data/raw/link\", mag, year)) as f:\n",
    "            input_linked = json.load(f)\n",
    "        with open(os.path.join(\"data/ground-truth\", mag, year)) as f:\n",
    "            gt = json.load(f)\n",
    "        gt = clean_gt(gt)\n",
    "        gt_data += gt\n",
    "        input_linked = clean_raw(input_linked)\n",
    "\n",
    "        #due to non-determinism in the flair NER:\n",
    "        all_refs_gt = [g[\"page\"]+g[\"coord\"] for g in gt] \n",
    "        all_refs_linked = [ent[\"page\"]+ent[\"coord\"] for l in input_linked for ent in l]\n",
    "        all_valid_refs = set(all_refs_gt).intersection(set(all_refs_linked))\n",
    "\n",
    "        for ent_variations in input_linked:\n",
    "            for key in split:\n",
    "                ent_instances = []\n",
    "                for ent in ent_variations:\n",
    "                    if ent[\"page\"] in split[key]:\n",
    "                        if (ent[\"page\"]+ent[\"coord\"]) in all_valid_refs:\n",
    "                            ent_instances.append({\"ent\": ent, \"label\": label_entity(ent, gt)})\n",
    "                if ent_instances:\n",
    "                    data[key].append(ent_instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from candidate_generation import create_metagrid_candidates, get_candidates_fuseki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load everytime you run this as we pop keys to keep data clean..\n",
    "with open(\"data/processed.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    ent_cand_label = []\n",
    "    i = 0\n",
    "    for entity_list in tqdm(data[split], smoothing=0.01):\n",
    "        i += 1\n",
    "        # Create candidates only for the first entry in the list as all the entity information is always the same\n",
    "        # The only thing that changes are pages and page_coordinates\n",
    "        candidates = create_metagrid_candidates(ent=entity_list[0][\"ent\"])\n",
    "        # Generate the list of page_coordinates and the corresponding labels!\n",
    "        coord_list = []\n",
    "        label_list = []\n",
    "        for ent_dict in entity_list:\n",
    "            ent = ent_dict[\"ent\"]\n",
    "            coord_list.append({\n",
    "                \"page\": ent.pop(\"page\", \"\"), \n",
    "                \"coords\": ent.pop(\"coord\", \"\")\n",
    "            })\n",
    "            label_list.append(ent_dict[\"label\"])\n",
    "        ent_cand_label.append({\"entity\": ent, \"candidates\": candidates, \"occurences\": coord_list, \"labels\": label_list})\n",
    "        if i % 100 == 0:\n",
    "            with open(f\"data/candidates/metagrid/candidates-gnd-{split}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(ent_cand_label, f)\n",
    "                    \n",
    "    with open(f\"data/candidates/metagrid/candidates-gnd-{split}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ent_cand_label, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To get the relevant fastttext model uncomment and run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.fasttext import FastText, load_facebook_vectors\n",
    "# model = load_facebook_vectors(\"cc.de.300.bin/cc.de.300.bin\")\n",
    "# model.save(\"./fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import  tqdm\n",
    "from feature_generation import candidates_to_features, process_fuseki_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load every time you run this as we pop keys to keep data clean..\n",
    "with open(\"data/processed.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "generator = \"metagrid\" # or \"fuseki\"\n",
    "\n",
    "problematic_entities = []\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    ent_cand_label = []\n",
    "    i = 0\n",
    "    for entity_list in tqdm(data[split], smoothing=0.01):\n",
    "        i += 1\n",
    "        # Create candidates only for the first entry in the list as all the entity information is always the same\n",
    "        # The only thing that changes are pages and page_coordinates\n",
    "        \n",
    "        # fuseki:\n",
    "        if generator == \"fuseki\":\n",
    "            unique_candidate_dict = get_candidates_fuseki(entity_list[0][\"ent\"])\n",
    "            candidates = process_fuseki_candidates(unique_candidate_dict)\n",
    "        \n",
    "        # metagrid:\n",
    "        if generator == \"metagrid\":\n",
    "            candidates = create_metagrid_candidates(ent=entity_list[0][\"ent\"])\n",
    "        \n",
    "        #Generate the list of page_coordinates and the corresponding labels!\n",
    "        coord_list = []\n",
    "        gt_label = []\n",
    "        for ent_dict in entity_list:\n",
    "            ent = ent_dict[\"ent\"]\n",
    "            coord_list.append({\n",
    "                \"page\": ent.pop(\"page\", \"\"), \n",
    "                \"coords\": ent.pop(\"coord\", \"\")\n",
    "            })\n",
    "            gt_label.append(ent_dict[\"label\"])\n",
    "\n",
    "        if len(gt_label)!=1:\n",
    "            problematic_entities.append({\"ent_list\": entity_list, \"gt_labels\": gt_label, \"mag\": coord_list})\n",
    "        gt_label = gt_label.pop()\n",
    "        \n",
    "        ent_cand_label.append({\"entity\": ent, \"candidates\": candidates, \"occurences\": coord_list, \"labels\": gt_label})\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            with open(f\"data/candidates/{generator}/candidates-gnd-{split}-{i}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(ent_cand_label, f)\n",
    "                    \n",
    "    with open(f\"data/candidates/{generator}/candidates-gnd-{split}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ent_cand_label, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  tqdm\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeature_generation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m candidates_to_features\n",
      "File \u001b[1;32m~\\git\\ETH-CS4NLP-22-Project-Linking-GND\\feature_generation.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcandidate_generation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_coords_from_candidate, extract_field_by_gnd\n\u001b[0;32m     12\u001b[0m loc \u001b[38;5;241m=\u001b[39m Nominatim(user_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetLoc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m ft \u001b[38;5;241m=\u001b[39m \u001b[43mFastTextKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./fasttext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_min_distance\u001b[39m(pairs):\n\u001b[0;32m     16\u001b[0m     distances \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs4nlp\\lib\\site-packages\\gensim\\models\\fasttext.py:1001\u001b[0m, in \u001b[0;36mFastTextKeyedVectors.load\u001b[1;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, fname_or_handle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a previously saved `FastTextKeyedVectors` model.\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \n\u001b[0;32m    985\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    999\u001b[0m \n\u001b[0;32m   1000\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(FastTextKeyedVectors, \u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mload(fname_or_handle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs4nlp\\lib\\site-packages\\gensim\\utils.py:486\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    482\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, fname)\n\u001b[0;32m    484\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[1;32m--> 486\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m obj\u001b[38;5;241m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[0;32m    488\u001b[0m obj\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs4nlp\\lib\\site-packages\\gensim\\utils.py:1460\u001b[0m, in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munpickle\u001b[39m(fname):\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \n\u001b[0;32m   1449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \n\u001b[0;32m   1459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1460\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _pickle\u001b[38;5;241m.\u001b[39mload(f, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs4nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs4nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './fasttext'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm import  tqdm\n",
    "from feature_generation import candidates_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = \"metagrid\" # or \"fuseki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"data/candidates/{generator}/candidates-gnd-{split}.pkl\", \"rb\") as f:\n",
    "        ent_cand_label = pickle.load(f)\n",
    "\n",
    "    list_of_good_entities = []\n",
    "    list_of_problematic_entities = []\n",
    "    for ent_dict in tqdm(ent_cand_label):\n",
    "        if len(set(ent_dict[\"labels\"])) > 1:\n",
    "            for label in set(ent_dict[\"labels\"]):\n",
    "                ent_dict[\"label\"] = label\n",
    "                features = candidates_to_features(ent=ent_dict[\"entity\"], candidates=ent_dict[\"candidates\"], gt_label=ent_dict[\"label\"])\n",
    "                ent_dict.update(features)\n",
    "                list_of_problematic_entities.append(ent_dict.copy())\n",
    "        else:\n",
    "            ent_dict[\"label\"] = set(ent_dict[\"labels\"]).pop()\n",
    "            features = candidates_to_features(ent=ent_dict[\"entity\"], candidates=ent_dict[\"candidates\"], gt_label=ent_dict[\"label\"])\n",
    "            ent_dict.update(features)\n",
    "            list_of_good_entities.append(ent_dict)\n",
    "            \n",
    "    with open(f\"data/features/{generator}/{split}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list_of_good_entities, file=f)\n",
    "    \n",
    "    with open(f\"data/features/{generator}/{split}_problematic.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list_of_problematic_entities, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from evaluation import perform_experiment, crossvalidate_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = \"metagrid\" # or \"fuseki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"data/features/{generator}/{split}.pkl\", \"rb\") as f:\n",
    "        d[split] = pickle.load(file=f)\n",
    "\n",
    "# problematic entities\n",
    "d_problem = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"data/features/{generator}/{split}_problematic.pkl\", \"rb\") as f:\n",
    "        d_problem[split] = pickle.load(file=f)\n",
    "\n",
    "d_combined = {\"train\": d[\"train\"] + d_problem[\"train\"], \"eval\": d[\"eval\"] + d_problem[\"eval\"], \"test\": d[\"test\"] + d_problem[\"test\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best scores we could get**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_scores, ment_scores = perform_experiment(\n",
    "    keep_empty=True,\n",
    "    do_sample=True,\n",
    "    oversampling=2, # Multiple of how often we oversample y = 1\n",
    "    balance=3, # multiple of y = 0 samples vs y = 1 samples\n",
    "    train=d[\"train\"] + d[\"eval\"],\n",
    "    eval=d[\"test\"],\n",
    "    model=ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True),\n",
    "    n_s=[1,10], # How many candidates do we keep\n",
    "    thresholds=[0.01, 0.2], # Where do we cut off\n",
    "    verbose=False # Print stuff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.356 RE:  0.216 PR: 1.0 AC: 0.426\n",
      "TP: 32 FN: 116 FP 0 TN 54\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.2\n",
      "F1: 0.346 RE:  0.209 PR: 1.0 AC: 0.421\n",
      "TP: 31 FN: 117 FP 0 TN 54\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.365 RE:  0.223 PR: 1.0 AC: 0.431\n",
      "TP: 33 FN: 115 FP 0 TN 54\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.2\n",
      "F1: 0.346 RE:  0.209 PR: 1.0 AC: 0.421\n",
      "TP: 31 FN: 117 FP 0 TN 54\n",
      "\n",
      "\n",
      "Mention Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.406 RE:  0.255 PR: 1.0 AC: 0.487\n",
      "TP: 116 FN: 339 FP 0 TN 206\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.2\n",
      "F1: 0.401 RE:  0.251 PR: 1.0 AC: 0.484\n",
      "TP: 114 FN: 341 FP 0 TN 206\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.412 RE:  0.259 PR: 1.0 AC: 0.49\n",
      "TP: 118 FN: 337 FP 0 TN 206\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.2\n",
      "F1: 0.401 RE:  0.251 PR: 1.0 AC: 0.484\n",
      "TP: 114 FN: 341 FP 0 TN 206\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEntity Level\")\n",
    "for score in ent_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()\n",
    "\n",
    "print(\"Mention Level\")\n",
    "for score in ment_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including problematic entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_scores, ment_scores = perform_experiment(\n",
    "    keep_empty=True,\n",
    "    do_sample=True,\n",
    "    oversampling=2, # Multiple of how often we oversample y = 1\n",
    "    balance=3, # multiple of y = 0 samples vs y = 1 samples\n",
    "    train=d_combined[\"train\"] + d_combined[\"eval\"],\n",
    "    eval=d_combined[\"test\"],\n",
    "    model=ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True),\n",
    "    n_s=[1,10], # How many candidates do we keep\n",
    "    thresholds=[0.01, 0.2], # Where do we cut off\n",
    "    verbose=False # Print stuff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.325 RE:  0.194 PR: 1.0 AC: 0.403\n",
      "TP: 34 FN: 141 FP 0 TN 61\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.2\n",
      "F1: 0.317 RE:  0.189 PR: 1.0 AC: 0.398\n",
      "TP: 33 FN: 142 FP 0 TN 61\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.333 RE:  0.2 PR: 1.0 AC: 0.407\n",
      "TP: 35 FN: 140 FP 0 TN 61\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.2\n",
      "F1: 0.325 RE:  0.194 PR: 1.0 AC: 0.403\n",
      "TP: 34 FN: 141 FP 0 TN 61\n",
      "\n",
      "\n",
      "Mention Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.632 RE:  0.462 PR: 1.0 AC: 0.567\n",
      "TP: 440 FN: 513 FP 0 TN 232\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.2\n",
      "F1: 0.631 RE:  0.461 PR: 1.0 AC: 0.566\n",
      "TP: 439 FN: 514 FP 0 TN 232\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.638 RE:  0.468 PR: 1.0 AC: 0.572\n",
      "TP: 446 FN: 507 FP 0 TN 232\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.2\n",
      "F1: 0.637 RE:  0.467 PR: 1.0 AC: 0.571\n",
      "TP: 445 FN: 508 FP 0 TN 232\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEntity Level\")\n",
    "for score in ent_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()\n",
    "\n",
    "print(\"Mention Level\")\n",
    "for score in ment_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform crossvalidation experiments like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ent_scores, ment_scores = crossvalidate_experiment(\n",
    "    train=d_combined[\"train\"], # + d_combined[\"eval\"],\n",
    "    eval=d_combined[\"eval\"],\n",
    "    n_fold = 5,\n",
    "    keep_empty=True,\n",
    "    do_sample=True,\n",
    "    oversampling=2, # Multiple of how often we oversample y = 1\n",
    "    balance=3, # multiple of y = 0 samples vs y = 1 samples\n",
    "    model=ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True),\n",
    "    n_s=[1,10], # How many candidates do we keep\n",
    "    thresholds=[0.01, 0.1, 0.3, 0.5], # Where do we cut off\n",
    "    verbose=False # Print stuff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.484 RE:  0.324 PR: 0.958 AC: 0.608\n",
      "TP: 23 FN: 48 FP 1 TN 53\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.1\n",
      "F1: 0.479 RE:  0.319 PR: 0.958 AC: 0.603\n",
      "TP: 23 FN: 49 FP 1 TN 53\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.3\n",
      "F1: 0.463 RE:  0.306 PR: 0.957 AC: 0.595\n",
      "TP: 22 FN: 50 FP 1 TN 53\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.5\n",
      "F1: 0.447 RE:  0.292 PR: 0.955 AC: 0.587\n",
      "TP: 21 FN: 51 FP 1 TN 53\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.545 RE:  0.375 PR: 1.0 AC: 0.643\n",
      "TP: 27 FN: 45 FP 0 TN 54\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.1\n",
      "F1: 0.531 RE:  0.361 PR: 1.0 AC: 0.635\n",
      "TP: 26 FN: 46 FP 0 TN 54\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.3\n",
      "F1: 0.5 RE:  0.333 PR: 1.0 AC: 0.619\n",
      "TP: 24 FN: 48 FP 0 TN 54\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.5\n",
      "F1: 0.484 RE:  0.319 PR: 1.0 AC: 0.611\n",
      "TP: 23 FN: 49 FP 0 TN 54\n",
      "\n",
      "\n",
      "Mention Level\n",
      "N: 1 Threshold: 0.01\n",
      "F1: 0.626 RE:  0.459 PR: 0.983 AC: 0.649\n",
      "TP: 119 FN: 140 FP 2 TN 144\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.1\n",
      "F1: 0.626 RE:  0.459 PR: 0.983 AC: 0.649\n",
      "TP: 119 FN: 140 FP 2 TN 144\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.3\n",
      "F1: 0.615 RE:  0.448 PR: 0.983 AC: 0.642\n",
      "TP: 116 FN: 143 FP 2 TN 144\n",
      "\n",
      "\n",
      "N: 1 Threshold: 0.5\n",
      "F1: 0.608 RE:  0.44 PR: 0.983 AC: 0.637\n",
      "TP: 114 FN: 145 FP 2 TN 144\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.01\n",
      "F1: 0.668 RE:  0.502 PR: 1.0 AC: 0.682\n",
      "TP: 130 FN: 129 FP 0 TN 147\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.1\n",
      "F1: 0.658 RE:  0.49 PR: 1.0 AC: 0.675\n",
      "TP: 127 FN: 132 FP 0 TN 147\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.3\n",
      "F1: 0.64 RE:  0.471 PR: 1.0 AC: 0.663\n",
      "TP: 122 FN: 137 FP 0 TN 147\n",
      "\n",
      "\n",
      "N: 10 Threshold: 0.5\n",
      "F1: 0.633 RE:  0.463 PR: 1.0 AC: 0.658\n",
      "TP: 120 FN: 139 FP 0 TN 147\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEntity Level\")\n",
    "for score in ent_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()\n",
    "\n",
    "print(\"Mention Level\")\n",
    "for score in ment_scores:\n",
    "    print(\"N:\", score[\"top_n\"], \"Threshold:\", score[\"threshold\"])\n",
    "    score[\"score\"].print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "models = [\n",
    "    GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='squared_error'),\n",
    "    ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True),\n",
    "    ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"absolute_error\", bootstrap=True),\n",
    "    ExtraTreesClassifier(n_estimators=100, random_state=0, bootstrap=True),\n",
    "    ElasticNet(random_state=0),\n",
    "    GaussianProcessRegressor(kernel = DotProduct() + WhiteKernel(), random_state=0)\n",
    "]\n",
    "\n",
    "model_names = [\"Gradient Boosting Regressor\", \"Extra Trees Regressor Squared\", \"Extra Trees Regressor Absolute\", \"Extra Trees Classifier\", \"Elastic Net\", \"Gaussian Process\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_list = [1, 2, 3, 5]\n",
    "oversampling_list = [1, 2, 3, 5]\n",
    "n_s = [1,10]\n",
    "thresholds = [0.01, 0.1, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_detailed = []\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "    print(model_name)\n",
    "    results = []\n",
    "    for data, data_name in zip([d, d_combined], [\"cleaned\", \"combined\"]):\n",
    "        print(\"Data:\", data_name)\n",
    "        for keep_empty in [True, False]:\n",
    "            print(\"keep_empty:\", keep_empty)\n",
    "            for do_sample in [True, False]:\n",
    "                print(\"do_sample:\", do_sample)\n",
    "                if do_sample:\n",
    "                    for balance in balance_list:\n",
    "                        print(\"Balance:\", balance)\n",
    "                        for oversampling in oversampling_list:\n",
    "                            print(\"Oversampling:\", oversampling)\n",
    "                            ent_scores, ment_scores = crossvalidate_experiment(\n",
    "                                    train = data[\"train\"],\n",
    "                                    eval = data[\"eval\"],\n",
    "                                    n_fold = 5,\n",
    "                                    keep_empty=keep_empty,\n",
    "                                    do_sample=do_sample, \n",
    "                                    oversampling=oversampling, \n",
    "                                    balance=balance,\n",
    "                                    model=model,\n",
    "                                    n_s=n_s,\n",
    "                                    thresholds=thresholds,\n",
    "                                    verbose=False \n",
    "                                )\n",
    "                            results.append({\n",
    "                                \"keep_empty\": keep_empty,\n",
    "                                \"do_sample\": do_sample,\n",
    "                                \"balance\": balance,\n",
    "                                \"oversampling\": oversampling,\n",
    "                                \"ent_scores\": ent_scores,\n",
    "                                \"ment_scores\": ment_scores,\n",
    "                                \"model\": model_name,\n",
    "                                \"data\": data_name\n",
    "                            })\n",
    "                else:\n",
    "                    balance = 1\n",
    "                    oversampling = 1 \n",
    "                    ent_scores, ment_scores = crossvalidate_experiment(\n",
    "                            train = data[\"train\"],\n",
    "                            eval = data[\"eval\"],\n",
    "                            n_fold = 5,\n",
    "                            keep_empty=keep_empty,\n",
    "                            do_sample=do_sample, \n",
    "                            oversampling=oversampling, \n",
    "                            balance=balance,\n",
    "                            model=model,\n",
    "                            n_s=n_s,\n",
    "                            thresholds=thresholds,\n",
    "                            verbose=False \n",
    "                        )\n",
    "                    results.append({\n",
    "                        \"keep_empty\": keep_empty,\n",
    "                        \"do_sample\": do_sample,\n",
    "                        \"balance\": balance,\n",
    "                        \"oversampling\": oversampling,\n",
    "                        \"ent_scores\": ent_scores,\n",
    "                        \"ment_scores\": ment_scores,\n",
    "                        \"model\": model_name,\n",
    "                        \"data\": data_name\n",
    "                    })\n",
    "    model_results_detailed.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parameter_tuning/model_results_detailed.pkl', 'wb') as out:\n",
    "     pickle.dump(model_results_detailed, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10 scores are necessarily better than top_1 scores, here we can decide which one we choose to get metrics about\n",
    "top_n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(scores, score_name, dict, current_setup, top_n):\n",
    "    for score in scores:\n",
    "        score_dict = score[\"score\"].get_score()\n",
    "        if score[\"top_n\"] == top_n:\n",
    "            curr_setup[\"top_n\"] = score[\"top_n\"]\n",
    "            curr_setup[\"threshold\"] = score[\"threshold\"]\n",
    "            if score_dict[score_name] > dict[f\"top_{score_name}\"]:\n",
    "                dict[f\"top_{score_name}\"] = score_dict[score_name]\n",
    "                dict[f\"top_{score_name}_setup\"] = [current_setup]\n",
    "            elif score_dict[score_name] == dict[f\"top_{score_name}\"]:\n",
    "                dict[f\"top_{score_name}_setup\"].append(current_setup)\n",
    "    return dict\n",
    "\n",
    "model_results = []\n",
    "for model_name in model_names:\n",
    "    dictionary={\n",
    "        \"ent\": {\n",
    "            \"top_F1\": 0,\n",
    "            \"top_Recall\": 0,\n",
    "            \"top_Precision\": 0,\n",
    "            \"top_F1_setup\": [],\n",
    "            \"top_Recall_setup\": [],\n",
    "            \"top_Precision_setup\": []\n",
    "        },\n",
    "        \"ment\": {\n",
    "            \"top_F1\": 0,\n",
    "            \"top_Recall\": 0,\n",
    "            \"top_Precision\": 0,\n",
    "            \"top_F1_setup\": [],\n",
    "            \"top_Recall_setup\": [],\n",
    "            \"top_Precision_setup\": []\n",
    "        }\n",
    "    }\n",
    "    for di in model_results_detailed:\n",
    "        if di[\"model\"] == model_name:\n",
    "            for scoring_level in [\"ent\", \"ment\"]:\n",
    "                scores = di[f\"{scoring_level}_scores\"]\n",
    "                curr_setup = {\n",
    "                    \"data\": di[\"data\"],\n",
    "                    \"do_sample\": di[\"do_sample\"],\n",
    "                    \"balance\": di[\"balance\"],\n",
    "                    \"oversampling\": di[\"oversampling\"],\n",
    "                    \"keep_empty\": di[\"keep_empty\"]\n",
    "                }\n",
    "                for score_name in [\"F1\", \"Recall\", \"Precision\"]:\n",
    "                    dictionary[scoring_level] = extract(scores, score_name, dictionary[scoring_level], curr_setup, top_n=top_n )\n",
    "    model_results.append(dictionary)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = \"F1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scores, model_name in zip(model_results, model_names):\n",
    "    balance = []\n",
    "    combined = []\n",
    "    oversampling = []\n",
    "    keep_empty = []\n",
    "    do_sample = []\n",
    "    top_n = []\n",
    "    thresholds = []\n",
    "    print(model_name)\n",
    "    for scoring_level in [\"ent\", \"ment\"]:\n",
    "        print(f\"{scoring_level}\\t\", f\"{score}:\\t\", scores[scoring_level][f\"top_{score}\"])\n",
    "        print(f\"{scoring_level}\\t\", f\"{score} Setup:\")\n",
    "        for setup in scores[scoring_level][f\"top_{score}_setup\"]:\n",
    "            if setup[\"data\"] == \"cleaned\":\n",
    "                combined.append(0)\n",
    "            else:\n",
    "                combined.append(1)\n",
    "            balance.append(setup[\"balance\"])\n",
    "            oversampling.append(setup[\"oversampling\"])\n",
    "            top_n.append(setup[\"top_n\"])\n",
    "            thresholds.append(setup[\"threshold\"])\n",
    "            if setup[\"do_sample\"]:\n",
    "                do_sample.append(1)\n",
    "            else:\n",
    "                do_sample.append(0)\n",
    "            if setup[\"keep_empty\"]:\n",
    "                keep_empty.append(1)\n",
    "            else:\n",
    "                keep_empty.append(0)\n",
    "    \n",
    "        print(f\"Number of setups: {len(do_sample)}\")\n",
    "\n",
    "        print(\"Mean\")\n",
    "        print(\"sample\", np.mean(do_sample))\n",
    "        print(\"empty \", np.mean(keep_empty))\n",
    "        print(\"combin\", np.mean(combined))\n",
    "        print(\"tresh \", np.mean(thresholds))\n",
    "        print(\"top_n \", np.mean(top_n))\n",
    "        print(\"overs \", np.mean(oversampling))\n",
    "        print(\"balanc\", np.mean(balance))\n",
    "\n",
    "        print(\"Median\")\n",
    "        print(\"sample\", np.median(do_sample))\n",
    "        print(\"empty \", np.median(keep_empty))\n",
    "        print(\"combin\", np.median(combined))\n",
    "        print(\"tresh \", np.median(thresholds))\n",
    "        print(\"top_n \", np.median(top_n))\n",
    "        print(\"overs \", np.median(oversampling))\n",
    "        print(\"balanc\", np.median(balance))\n",
    "        print(\"\\n\\n\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6d2888e1739c5fff784269431c26df89048cab5e87b70446ac408621b178c3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
