{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from preprocessing import clean_gt, clean_raw, label_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['sbz-002_1940_115_0223.txt',\n",
       "  'dkm-001_1941_001_0431.txt',\n",
       "  'dkm-001_1941_001_0438.txt',\n",
       "  'dkm-001_1941_001_0441.txt',\n",
       "  'sbz-002_1940_115_0222.txt',\n",
       "  'sbz-002_1965_083_3175.txt',\n",
       "  'sbz-002_1940_115_0224.txt',\n",
       "  'sbz-002_1895_025_0676.txt',\n",
       "  'sbz-002_1940_115_0218.txt',\n",
       "  'sbz-002_1895_025_0679.txt',\n",
       "  'dkm-003_2010_070_0050.txt',\n",
       "  'sbz-002_1940_115_0221.txt',\n",
       "  'sbz-004_2010_136_0884.txt',\n",
       "  'sbz-002_1940_115_0220.txt',\n",
       "  'sbz-002_1965_083_3184.txt',\n",
       "  'sbz-002_1895_025_0678.txt',\n",
       "  'sbz-002_1965_083_3188.txt',\n",
       "  'dkm-001_1941_001_0423.txt',\n",
       "  'sbz-004_2010_136_0887.txt',\n",
       "  'sbz-004_2010_136_0890.txt',\n",
       "  'dkm-003_2010_070_0053.txt',\n",
       "  'dkm-001_1941_001_0434.txt',\n",
       "  'sbz-004_2010_136_0883.txt',\n",
       "  'sbz-004_2010_136_0882.txt',\n",
       "  'sbz-002_1965_083_3178.txt',\n",
       "  'dkm-001_1941_001_0430.txt',\n",
       "  'dkm-003_2010_070_0056.txt',\n",
       "  'sbz-004_2010_136_0885.txt',\n",
       "  'sbz-002_1965_083_3173.txt',\n",
       "  'sbz-002_1965_083_3172.txt',\n",
       "  'sbz-002_1965_083_3180.txt',\n",
       "  'dkm-001_1941_001_0433.txt',\n",
       "  'sbz-002_1965_083_3167.txt',\n",
       "  'dkm-001_1941_001_0443.txt',\n",
       "  'dkm-001_1941_001_0437.txt',\n",
       "  'sbz-004_2010_136_0880.txt',\n",
       "  'dkm-003_2010_070_0059.txt',\n",
       "  'dkm-003_2010_070_0051.txt',\n",
       "  'sbz-004_2010_136_0896.txt',\n",
       "  'dkm-003_2010_070_0049.txt',\n",
       "  'sbz-002_1965_083_3183.txt',\n",
       "  'dkm-003_2010_070_0048.txt',\n",
       "  'sbz-004_2010_136_0878.txt',\n",
       "  'sbz-004_2010_136_0897.txt',\n",
       "  'dkm-003_2010_070_0045.txt',\n",
       "  'sbz-004_2010_136_0889.txt',\n",
       "  'sbz-002_1965_083_3171.txt',\n",
       "  'dkm-001_1941_001_0427.txt',\n",
       "  'dkm-003_2010_070_0047.txt',\n",
       "  'sbz-004_2010_136_0894.txt',\n",
       "  'dkm-001_1941_001_0425.txt',\n",
       "  'dkm-003_2010_070_0042.txt',\n",
       "  'sbz-002_1940_115_0219.txt',\n",
       "  'sbz-002_1965_083_3181.txt',\n",
       "  'dkm-001_1941_001_0440.txt',\n",
       "  'sbz-002_1940_115_0214.txt',\n",
       "  'sbz-004_2010_136_0892.txt',\n",
       "  'sbz-002_1895_025_0682.txt',\n",
       "  'sbz-002_1965_083_3186.txt',\n",
       "  'dkm-001_1941_001_0436.txt',\n",
       "  'dkm-003_2010_070_0052.txt',\n",
       "  'dkm-001_1941_001_0439.txt',\n",
       "  'sbz-002_1965_083_3174.txt',\n",
       "  'sbz-002_1965_083_3187.txt',\n",
       "  'sbz-002_1965_083_3168.txt',\n",
       "  'dkm-003_2010_070_0055.txt',\n",
       "  'dkm-003_2010_070_0057.txt',\n",
       "  'dkm-001_1941_001_0424.txt',\n",
       "  'sbz-002_1965_083_3169.txt',\n",
       "  'dkm-001_1941_001_0428.txt',\n",
       "  'dkm-001_1941_001_0429.txt',\n",
       "  'dkm-001_1941_001_0435.txt',\n",
       "  'sbz-002_1965_083_3177.txt',\n",
       "  'dkm-003_2010_070_0041.txt',\n",
       "  'sbz-002_1940_115_0213.txt',\n",
       "  'dkm-003_2010_070_0046.txt',\n",
       "  'sbz-002_1940_115_0217.txt',\n",
       "  'sbz-002_1895_025_0675.txt',\n",
       "  'sbz-002_1965_083_3185.txt',\n",
       "  'sbz-004_2010_136_0888.txt',\n",
       "  'sbz-004_2010_136_0895.txt',\n",
       "  'dkm-001_1941_001_0426.txt',\n",
       "  'dkm-001_1941_001_0432.txt',\n",
       "  'dkm-003_2010_070_0044.txt',\n",
       "  'sbz-002_1895_025_0683.txt',\n",
       "  'sbz-002_1965_083_3182.txt',\n",
       "  'dkm-001_1941_001_0442.txt',\n",
       "  'sbz-004_2010_136_0893.txt',\n",
       "  'dkm-003_2010_070_0043.txt',\n",
       "  'sbz-004_2010_136_0879.txt',\n",
       "  'sbz-004_2010_136_0881.txt',\n",
       "  'sbz-002_1895_025_0681.txt',\n",
       "  'sbz-002_1965_083_3170.txt',\n",
       "  'sbz-002_1965_083_3179.txt'],\n",
       " 'test': ['sbz-003_1990_108_1800.txt',\n",
       "  'dkm-003_1990_050_0388.txt',\n",
       "  'dkm-003_1990_050_0380.txt',\n",
       "  'sbz-003_1990_108_1798.txt',\n",
       "  'dkm-003_1990_050_0383.txt',\n",
       "  'sbz-003_1990_108_1796.txt',\n",
       "  'dkm-003_1990_050_0397.txt',\n",
       "  'dkm-003_1990_050_0393.txt',\n",
       "  'dkm-003_1990_050_0385.txt',\n",
       "  'dkm-003_1990_050_0386.txt',\n",
       "  'dkm-003_1990_050_0391.txt',\n",
       "  'dkm-003_1990_050_0389.txt',\n",
       "  'dkm-003_1990_050_0382.txt',\n",
       "  'sbz-003_1990_108_1794.txt',\n",
       "  'dkm-003_1990_050_0399.txt',\n",
       "  'dkm-003_1990_050_0381.txt',\n",
       "  'sbz-003_1990_108_1795.txt',\n",
       "  'dkm-003_1990_050_0392.txt',\n",
       "  'dkm-003_1990_050_0384.txt',\n",
       "  'sbz-003_1990_108_1801.txt',\n",
       "  'dkm-003_1990_050_0395.txt',\n",
       "  'dkm-003_1990_050_0400.txt',\n",
       "  'dkm-003_1990_050_0396.txt',\n",
       "  'sbz-003_1990_108_1793.txt',\n",
       "  'sbz-003_1990_108_1797.txt',\n",
       "  'dkm-003_1990_050_0394.txt',\n",
       "  'sbz-003_1990_108_1799.txt',\n",
       "  'dkm-003_1990_050_0398.txt',\n",
       "  'dkm-003_1990_050_0387.txt',\n",
       "  'dkm-003_1990_050_0390.txt'],\n",
       " 'eval': ['sbz-002_1895_025_0680.txt',\n",
       "  'dkm-003_2010_070_0054.txt',\n",
       "  'sbz-002_1940_115_0216.txt',\n",
       "  'dkm-003_2010_070_0040.txt',\n",
       "  'sbz-004_2010_136_0886.txt',\n",
       "  'sbz-002_1940_115_0215.txt',\n",
       "  'sbz-002_1965_083_3176.txt',\n",
       "  'sbz-004_2010_136_0891.txt',\n",
       "  'dkm-003_2010_070_0058.txt',\n",
       "  'sbz-002_1895_025_0677.txt']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = pickle.load(open('data/train_test_eval_filenames_new.pkl', 'rb'))\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train = 94, test = 30, eval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import random\n",
    "new_split = {\"train\":[], \"test\": [], \"eval\": []}\n",
    "for key in split:\n",
    "    for page in split[key]:\n",
    "        mag = page.split(\"_\")[0].split(\"-\")[0]\n",
    "        year = page.split(\"_\")[1]\n",
    "        \n",
    "        if mag == \"dkm\" and (year == \"1941\" or year == \"2010\"):\n",
    "            new_split[\"train\"].append(page)\n",
    "        if (year == \"1990\"):\n",
    "            new_split[\"test\"].append(page)\n",
    "        if mag ==\"sbz\" and (year == \"1895\" or year == \"1940\" or year == \"1965\" or year == \"2010\"):\n",
    "            new_split[\"train\"].append(page)\n",
    "\n",
    "eval_set = random.sample(new_split[\"train\"], int(len(new_split[\"train\"])/10)) #set 10% of train aside for eval\n",
    "for page in eval_set:\n",
    "    new_split[\"train\"].remove(page)\n",
    "new_split[\"eval\"] = eval_set\n",
    "\n",
    "with open('train_test_eval_filenames_new.pkl', 'wb') as out:\n",
    "    pickle.dump(new_split, out)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"train\": [],\n",
    "    \"test\": [],\n",
    "    \"eval\": []\n",
    "}\n",
    "gt_data = []\n",
    "for mag in [\"dkm\", \"sbz\"]:\n",
    "    for year in os.listdir(f'data/raw/link/{mag}'):\n",
    "        with open(os.path.join(\"data/raw/link\", mag, year)) as f:\n",
    "            input_linked = json.load(f)\n",
    "        with open(os.path.join(\"data/ground-truth\", mag, year)) as f:\n",
    "            gt = json.load(f)\n",
    "        gt = clean_gt(gt)\n",
    "        gt_data += gt\n",
    "        input_linked = clean_raw(input_linked)\n",
    "\n",
    "        #due to non-determinism in the flair NER:\n",
    "        all_refs_gt = [g[\"page\"]+g[\"coord\"] for g in gt] \n",
    "        all_refs_linked = [ent[\"page\"]+ent[\"coord\"] for l in input_linked for ent in l]\n",
    "        all_valid_refs = set(all_refs_gt).intersection(set(all_refs_linked))\n",
    "\n",
    "        for ent_variations in input_linked:\n",
    "            for key in split:\n",
    "                ent_instances = []\n",
    "                for ent in ent_variations:\n",
    "                    if ent[\"page\"] in split[key]:\n",
    "                        if (ent[\"page\"]+ent[\"coord\"]) in all_valid_refs:\n",
    "                            ent_instances.append({\"ent\": ent, \"label\": label_entity(ent, gt)})\n",
    "                if ent_instances:\n",
    "                    data[key].append(ent_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from candidate_generation import create_metagrid_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8438321c2af4da49a430b66636c2274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load everytime you run this as we pop keys to keep data clean..\n",
    "with open(\"data/processed.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    ent_cand_label = []\n",
    "    i = 0\n",
    "    for entity_list in tqdm(data[split], smoothing=0.01):\n",
    "        i += 1\n",
    "        # Create candidates only for the first entry in the list as all the entity information is always the same\n",
    "        # The only thing that changes are pages and page_coordinates\n",
    "        candidates = create_metagrid_candidates(ent=entity_list[0][\"ent\"])\n",
    "        # Generate the list of page_coordinates and the corresponding labels!\n",
    "        coord_list = []\n",
    "        label_list = []\n",
    "        for ent_dict in entity_list:\n",
    "            ent = ent_dict[\"ent\"]\n",
    "            coord_list.append({\n",
    "                \"page\": ent.pop(\"page\", \"\"), \n",
    "                \"coords\": ent.pop(\"coord\", \"\")\n",
    "            })\n",
    "            label_list.append(ent_dict[\"label\"])\n",
    "        ent_cand_label.append({\"entity\": ent, \"candidates\": candidates, \"occurences\": coord_list, \"labels\": label_list})\n",
    "        if i % 100 == 0:\n",
    "            with open(f\"candidates-gnd-{split}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(ent_cand_label, f)\n",
    "                    \n",
    "    with open(f\"candidates-gnd-{split}.pkl\", \"wb\") as f:\n",
    "       pickle.dump(ent_cand_label, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText, load_facebook_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_facebook_vectors(\"cc.de.300.bin/cc.de.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"./fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import  tqdm\n",
    "from feature_generation import create_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a6eb7711a241f7879283b9df904ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f777d494bdd4a63b7783f9bf744d43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f190af4c75624483ae6f05ca09e5e570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split in [\"train\", \"eval\", \"test\"]:\n",
    "    with open(f\"candidates-gnd-{split}.pkl\", \"rb\") as f:\n",
    "        ent_cand_label = pickle.load(f)\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for ent_dict in tqdm(ent_cand_label):\n",
    "        # The entity in the list is always the same only the coordinates change!\n",
    "        # candidate_entity_features = []\n",
    "        feature_list = []\n",
    "        for candidate in ent_dict[\"candidates\"]:\n",
    "            features = create_features(ent_dict[\"entity\"], candidate)\n",
    "            feature_list.append(features)\n",
    "            # candidate_entity_features.append([get_gnd(candidate)] + features)\n",
    "        ent_dict[\"features\"] = feature_list\n",
    "    with open(f\"data/features/{split}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(ent_cand_label, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from feature_generation import get_gnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"train\": {}, \"eval\": {}}\n",
    "for split in [\"train\", \"eval\"]:\n",
    "    for filename in [\"y\", \"X\"]:\n",
    "        with open(f\"data/features/{split}/{filename}.pkl\", \"rb\") as f:\n",
    "            d[split][filename] = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"train\": {}, \"eval\": {}}\n",
    "for split in [\"train\", \"eval\"]:\n",
    "    with open(f\"data/features/{split}.pkl\", \"rb\") as f:\n",
    "        d[split] = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training(entity_dict_list):\n",
    "    X = []\n",
    "    y = []\n",
    "    X_ent = []\n",
    "    y_ent = []\n",
    "    for ent_dict in entity_dict_list:\n",
    "        ent = ent_dict[\"entity\"]\n",
    "        labels = ent_dict[\"labels\"]\n",
    "        cand_gnds = [get_gnd(x) for x in ent_dict[\"candidates\"]]\n",
    "        \n",
    "        for coocurence, label in zip(ent_dict[\"occurences\"], ent_dict[\"labels\"]):\n",
    "            for features, candidate in zip(ent_dict[\"features\"], ent_dict[\"candidates\"]):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "y_test_ids = []\n",
    "x_test_ids = []\n",
    "\n",
    "for X, y in zip(d[\"train\"][\"X\"], d[\"train\"][\"y\"]):\n",
    "    # The features are always the same for a given entity...\n",
    "    features = X.pop()\n",
    "    for feature in features:\n",
    "        X_train.append(feature[1:])\n",
    "        if feature[0] == y[0]:\n",
    "            y_train.append(1)\n",
    "        else:\n",
    "            y_train.append(0)\n",
    "\n",
    "for x_ent, y_ent in zip(d[\"eval\"][\"X_ent\"], d[\"eval\"][\"y_ent\"]):\n",
    "    for x in x_ent:\n",
    "        X_test.append(x[1:])\n",
    "        if x[0] == y_ent[0]:\n",
    "            y_test.append(1)\n",
    "        else:\n",
    "            y_test.append(0)\n",
    "        x_test_ids.append(x[0])\n",
    "        y_test_ids.append(y_ent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/andreas/Polybox/Uni/FS22/Computational Semantics/ETH-CS4NLP-22-Project-Linking-GND/linking.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andreas/Polybox/Uni/FS22/Computational%20Semantics/ETH-CS4NLP-22-Project-Linking-GND/linking.ipynb#ch0000027?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m ExtraTreesRegressor\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andreas/Polybox/Uni/FS22/Computational%20Semantics/ETH-CS4NLP-22-Project-Linking-GND/linking.ipynb#ch0000027?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m ExtraTreesRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, criterion\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msquared_error\u001b[39m\u001b[39m\"\u001b[39m, bootstrap\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andreas/Polybox/Uni/FS22/Computational%20Semantics/ETH-CS4NLP-22-Project-Linking-GND/linking.ipynb#ch0000027?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "model = ExtraTreesRegressor(n_estimators=100, random_state=0, criterion=\"squared_error\", bootstrap=True)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(candidates, model):\n",
    "    candidate_scores = []\n",
    "    candidate_ids = []\n",
    "    for candidate in candidates:\n",
    "        features = candidate[1:]\n",
    "        score = model.predict(np.array(features).reshape(1,-1))[0]\n",
    "        candidate_scores.append(score)\n",
    "        candidate_ids.append(candidate[0])\n",
    "    if candidate_ids:\n",
    "        candidate_ids = np.array(candidate_ids)\n",
    "        indices = np.argsort(candidate_scores)\n",
    "        ids_sorted = np.array(candidate_ids)[indices[::-1]]\n",
    "        scores_sorted = np.array(candidate_scores)[indices[::-1]]\n",
    "        return ids_sorted, scores_sorted\n",
    "    else:\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mention Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e7e7f0238545fda49baf5c7f00d028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "y_ids = []\n",
    "for x_ent, y_ent in tqdm(zip(d[split][\"X_ent\"], d[split][\"y_ent\"])):\n",
    "    ids_local, scores_local = rank_candidates(x_ent, model)\n",
    "    ids.append(ids_local)\n",
    "    scores.append(scores_local)\n",
    "    y_ids.append(y_ent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1054380449', '1054380538', '126133123', '1208538853',\n",
       "       '1160560757', '129740101', '126133115', '1116500493', '1232438294',\n",
       "       '1063799058', '126133158', '1080667032', '1054380597',\n",
       "       '1054380562', '12613314X', '173450776', '1038629071', '111795834',\n",
       "       '105438049X', '1054380473', '126133131'], dtype='<U10')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = 0.000\n",
    "split = \"eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 28 23\n",
      "F1:           0.4516\n",
      "Recall:       0.4773\n",
      "Precision:    0.4286\n",
      "Accuracy:     0.7119\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "for ids_local, scores_local, y in zip(ids, scores, y_ids):\n",
    "    ids_local_thresh = [id_loc for id_loc, score_loc in zip(ids_local, scores_local) if score_loc > treshold]\n",
    "    #ids_local_thresh = ids_local\n",
    "    #print(ids_local)\n",
    "    if y in ids_local_thresh:\n",
    "        tp += 1\n",
    "    elif y == '' and ids_local_thresh != []:\n",
    "        fp += 1\n",
    "    elif y != '' and ids_local_thresh == []:\n",
    "        fn += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "\n",
    "print(tp, fp, fn)\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2* (precision * recall)/(precision+recall)\n",
    "print(\"F1:          \", round(f1,4))\n",
    "print(\"Recall:      \", round(recall,4))        \n",
    "print(\"Precision:   \", round(precision,4))\n",
    "print(\"Accuracy:    \", round(accuracy,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_prediction(ids, scores, cutoff_score=0.5, n_candidates=5):\n",
    "    count = 0\n",
    "    result = []\n",
    "    for id, score in zip(ids, scores):\n",
    "        count += 1\n",
    "        if count > n_candidates:\n",
    "            return result\n",
    "        else:\n",
    "            if score > cutoff_score:\n",
    "                result.append(id)\n",
    "            else:\n",
    "                result.append('')\n",
    "    if len(result) < 1 :\n",
    "        result.append('')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8572720d27841128e3bf4386f222ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "for x_ent in tqdm(d[split][\"X_ent\"]):\n",
    "    ids_local, scores_local = rank_candidates(x_ent, model)\n",
    "    ids.append(ids_local)\n",
    "    scores.append(scores_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = 0.01\n",
    "n = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:           0.5385\n",
      "Recall:       0.4118\n",
      "Precision:    0.7778\n",
      "Accuracy:     0.7966\n",
      "21 120 6 30\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for ids_local, scores_local in zip(ids, scores):\n",
    "    prediction = normalize_prediction(ids_local, scores_local, cutoff_score=treshold, n_candidates = n)\n",
    "    predictions.append(prediction)\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "\n",
    "for prediction, y in zip(predictions, d[split][\"y_ent\"]):\n",
    "    if y[0] == '':\n",
    "        if y[0] in prediction:\n",
    "            tn +=1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if y[0] in prediction:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "accuracy = (tp +tn)/(tp +tn +fp +fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2* (precision * recall)/(precision +recall)\n",
    "print(\"F1:          \", round(f1,4))\n",
    "print(\"Recall:      \", round(recall,4))        \n",
    "print(\"Precision:   \", round(precision,4))\n",
    "print(\"Accuracy:    \", round(accuracy,4))\n",
    "print(tp, tn, fp, fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6d2888e1739c5fff784269431c26df89048cab5e87b70446ac408621b178c3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
